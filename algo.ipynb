{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_coins\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Step 1: Fetch top 5000 coins by market cap\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m top_5000_coins \u001b[38;5;241m=\u001b[39m fetch_top_5000_coins()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 2: Extract coin IDs\u001b[39;00m\n\u001b[0;32m     31\u001b[0m coin_ids \u001b[38;5;241m=\u001b[39m [coin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m coin \u001b[38;5;129;01min\u001b[39;00m top_5000_coins]\n",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m, in \u001b[0;36mfetch_top_5000_coins\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m16\u001b[39m)  \u001b[38;5;66;03m# Sleep to avoid hitting rate limits\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_coins\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_top_5000_coins():\n",
    "    \"\"\"Fetch the top 5000 coins by market cap using the /coins/markets endpoint.\"\"\"\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets\"\n",
    "    all_coins = []\n",
    "    \n",
    "    for page in range(1, 21):  # 20 pages * 250 coins per page = 5000 coins\n",
    "        params = {\n",
    "            \"vs_currency\": \"usd\",\n",
    "            \"order\": \"market_cap_desc\",\n",
    "            \"category\": \"meme-token\",\n",
    "            \"per_page\": 250,\n",
    "            \"page\": page\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            all_coins.extend(response.json())\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        time.sleep(16)  # Sleep to avoid hitting rate limits\n",
    "    \n",
    "    return all_coins\n",
    "\n",
    "# Step 1: Fetch top 5000 coins by market cap\n",
    "top_5000_coins = fetch_top_5000_coins()\n",
    "\n",
    "# Step 2: Extract coin IDs\n",
    "coin_ids = [coin['id'] for coin in top_5000_coins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Descriptions for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 5000 coins with descriptions to CSV.\n"
     ]
    }
   ],
   "source": [
    "headers = {'x-cg-pro-api-key': \"CG-FtwnDTUpog7Z3GPYYqxXxQDb\"}\n",
    "def fetch_coin_descriptions(coin_ids):\n",
    "    \"\"\"Fetch descriptions for each coin using the /coins/{id} endpoint.\"\"\"\n",
    "    descriptions = {}\n",
    "    \n",
    "    for coin_id in coin_ids:\n",
    "        response = requests.get(f\"https://pro-api.coingecko.com/api/v3/coins/{coin_id}\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            descriptions[coin_id] = data['description']['en']\n",
    "        else:\n",
    "            print(f\"Failed to fetch description for {coin_id}: {response.status_code}\")\n",
    "        time.sleep(0.5)  # Sleep to avoid rate limits\n",
    "    \n",
    "    return descriptions\n",
    "coin_descriptions = fetch_coin_descriptions(coin_ids)\n",
    "\n",
    "# Step 4: Combine market cap data with descriptions\n",
    "for coin in top_5000_coins:\n",
    "    coin['description'] = coin_descriptions.get(coin['id'], \"\")\n",
    "\n",
    "# Step 5: Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(top_5000_coins)\n",
    "df = df.sort_values(by=\"market_cap\", ascending=False)  # Ensure sorting by market cap\n",
    "df.to_csv(\"top_5000_meme_coins.csv\", index=False)\n",
    "\n",
    "print(\"Saved top 5000 coins with descriptions to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dogecoin dogecoin cryptocurrency based popular doge internet meme features shiba inu logo dogecoin hrefcoinslitecoinlitecoina fork introduced joke currency december dogecoin quickly developed online community reached capitalization us million january compared cryptocurrencies dogecoin fast initial coin production schedule billion coins circulation mid additional billion coins every year thereafter june billionth dogecoin mined dogecoin created billy markus portland oregon jackson palmer sydney australia wanted create fun cryptocurrency appeal beyond core bitcoin audience dogecoin primarily used tipping system reddit twitter users tip creating sharing good content community active organising fundraising activities deserving causes developers dogecoin havent made major changes coin since means dogecoin could get left behind shibas leaving dogecoin join advanced platforms like ethereum one dogecoin strengths relaxed funloving community however also weakness currencies way professional purchase dogecoin involves downloading crypto wallet setting crypto exchange account trading away desired crypto currency set account doge currency exchange deposited funds ready start trading', 'Shiba Inu shiba inu shib meme token began fun currency transformed decentralized ecosystem initial launch supply allocated vitalik buterins ethereum wallet result vitalik proceeded donate shib holdings covid relief effort india remaining burnt forever donation worth billion time makes one largest donation ever world shiba inu community working right shiba inu team launched decentralized exchange called shibaswap new tokens leash bone leash scarce supply token used offer incentives shibaswap bone governance token holders vote proposals doggy dao', 'Pepe project pepe community based meme token surround iconic meme pepe frog pepe aims leverage power iconic meme become memeable memecoin existence makes project unique pepe make memecoins great ushering new paradigm memecoins pepe represents memecoin purest simplicity zero taxes liquidity locked forever contract immutable pepe people forever pepe culture rallying together community fun enjoy memes fueled purely memetic power history project pepe stealth launched friday april th whats next project pepe focus developing tightknit community around token building resources enrich communities knowledge success crypto token gated group newsletter tools token used pepe used speculate power memes pretend anything', 'Bonk bonk first solana dog coin people people total supply airdropped solana community bonk contributors tired toxic alameda tokenomics wanted make fun memecoin everyone gets fair shot', 'Pudgy Penguins pengu official coin pudgy penguins pudgy penguins become face crypto one influential communities industry large companies wearing penguin featured etf commercials garnering millions followers billion views pengu become cultural icon pengu allows everexpanding pudgy penguin fanbase hundreds millions people outside crypto see share pudgy penguin everyday join huddle pengu symbol community memes good vibes believe pengu believe prophecy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"top_5000_meme_coins.csv\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if type(text) is not str:\n",
    "        return ''\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]  \n",
    "    return ' '.join(tokens)\n",
    "df['processed_description'] = [preprocess_text(desc) for desc in df['description']]\n",
    "df['processed_description'] = df.apply(lambda row: f\"{row['name']} {row['processed_description']}\", axis=1)\n",
    "processed_descriptions = df['processed_description'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(processed_descriptions)\n",
    "embeddings = normalize(embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10, random_state=42)  # reduce dimensionality to decrease influence of common common words\n",
    "reduced_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters\n",
      "cluster 0: meme, coin, memecoin, token, community\n",
      "cluster 1: community, romeo, meme, token, crypto\n",
      "cluster 2: community, project, token, ecosystem, together\n",
      "cluster 3: loading, ily, right, pewpew, eyes\n",
      "cluster 4: meme, community, token, project, coin\n",
      "cluster 5: community, meme, token, solana, project\n",
      "cluster 6: blockchain, project, decentralized, users, ecosystem\n",
      "cluster 7: cat, community, meme, token, solana\n",
      "cluster 8: community, meme, project, token, blockchain\n",
      "cluster 9: community, project, token, blockchain, crypto\n",
      "cluster 10: ai, agent, agents, project, users\n",
      "cluster 11: meme, pepe, matt, frog, community\n",
      "cluster 12: token, community, doge, project, dog\n",
      "cluster 13: meme, crypto, coin, community, world\n",
      "cluster 14: kitty, keeps, spinning, cat, Cat\n",
      "cluster 15: meme, dog, community, doge, token\n",
      "cluster 16: meme, memecoin, pepe, memes, community\n",
      "cluster 17: token, community, tokens, project, meme\n",
      "cluster 18: token, coin, base, tokens, Coin\n",
      "cluster 19: dog, inu, doge, solana, Inu\n"
     ]
    }
   ],
   "source": [
    "df['cluster'] = labels  \n",
    "\n",
    "clustered_coins = df.groupby('cluster')\n",
    "\n",
    "cluster_keywords = {}\n",
    "for label, group in clustered_coins:\n",
    "    all_descriptions = ' '.join(group['processed_description'].tolist())\n",
    "    word_counts = Counter(all_descriptions.split())\n",
    "    common_words = [word for word, count in word_counts.most_common(5)]  # Top 5 common words\n",
    "    cluster_keywords[label] = common_words\n",
    "\n",
    "print(\"clusters\")\n",
    "for label, keywords in cluster_keywords.items():\n",
    "    print(f\"cluster {label}: {', '.join(keywords)}\")\n",
    "\n",
    "auto_meme_types = {\n",
    "    label: ' '.join(keywords) for label, keywords in cluster_keywords.items()\n",
    "}\n",
    "\n",
    "def get_meme_type(label):\n",
    "    return auto_meme_types.get(label, \"Unknown\")\n",
    "\n",
    "df['meme_type'] = df['cluster'].apply(get_meme_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
