{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_top_5000_coins():\n",
    "    \"\"\"Fetch the top 5000 coins by market cap using the /coins/markets endpoint.\"\"\"\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets\"\n",
    "    all_coins = []\n",
    "    \n",
    "    for page in range(1, 21):  # 20 pages * 250 coins per page = 5000 coins\n",
    "        params = {\n",
    "            \"vs_currency\": \"usd\",\n",
    "            \"order\": \"market_cap_desc\",\n",
    "            \"category\": \"meme-token\",\n",
    "            \"per_page\": 250,\n",
    "            \"page\": page\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            all_coins.extend(response.json())\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        time.sleep(16)  # Sleep to avoid hitting rate limits\n",
    "    \n",
    "    return all_coins\n",
    "\n",
    "# Step 1: Fetch top 5000 coins by market cap\n",
    "top_5000_coins = fetch_top_5000_coins()\n",
    "\n",
    "# Step 2: Extract coin IDs\n",
    "coin_ids = [coin['id'] for coin in top_5000_coins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Descriptions for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'x-cg-pro-api-key': \"CG-FtwnDTUpog7Z3GPYYqxXxQDb\"}\n",
    "def fetch_coin_descriptions(coin_ids):\n",
    "    \"\"\"Fetch descriptions for each coin using the /coins/{id} endpoint.\"\"\"\n",
    "    descriptions = {}\n",
    "    \n",
    "    for coin_id in coin_ids:\n",
    "        response = requests.get(f\"https://pro-api.coingecko.com/api/v3/coins/{coin_id}\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            descriptions[coin_id] = data['description']['en']\n",
    "        else:\n",
    "            print(f\"Failed to fetch description for {coin_id}: {response.status_code}\")\n",
    "        time.sleep(0.5)  # for rate limits\n",
    "    \n",
    "    return descriptions\n",
    "coin_descriptions = fetch_coin_descriptions(coin_ids)\n",
    "\n",
    "for coin in top_5000_coins:\n",
    "    coin['description'] = coin_descriptions.get(coin['id'], \"\")\n",
    "\n",
    "df = pd.DataFrame(top_5000_coins)\n",
    "df = df.sort_values(by=\"market_cap\", ascending=False)  \n",
    "df.to_csv(\"top_5000_meme_coins.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"top_5000_meme_coins.csv\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filler_words = set(['coin', 'solana', 'memes', 'cryptocurrency', 'crypto', 'token', 'project', 'meme', 'community', 'network', 'platform', 'and', 'or', 'it', 'etc', 'all', 'also', 'with', 'its', 'can', 'like', 'holders', 'ecosystem', 'chain', 'blockchain', 'meme'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if type(text) is not str:\n",
    "        return ''\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in filler_words]  \n",
    "    return ' '.join(tokens)\n",
    "df['processed_description'] = [preprocess_text(desc) for desc in df['description']]\n",
    "df['processed_description'] = df.apply(lambda row: f\"{row['name']} {row['processed_description']}\", axis=1)\n",
    "processed_descriptions = df['processed_description'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(processed_descriptions)\n",
    "embeddings = normalize(embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "categories = {\n",
    "    \"none\": [],\n",
    "    \"dog\": ['shiba', \"inu\", \"dog\", \"doge\", \"dogecoin\", \"shib\", \"shiba\", \"inu\"],\n",
    "    \"wif\": [\"wif\", \"hat\", \"wifhat\", \"dogwifhat\"],\n",
    "    \"pepe\": [\"pepe\", \"frog\", \"wojak\", \"kek\", \"green\", \"rare\"],\n",
    "    \"cat\": [\"cat\", \"kitty\", \"meow\", \"feline\"],\n",
    "    \"ai\": [\"ai\", \"artificial\", \"intelligence\", \"agent\"],\n",
    "    \"frog\": [\"frog\", \"ribbit\", \"amphibian\"],\n",
    "    \"fart_butt\": [\"fart\", \"butt\", \"gassy\", \"flatulence\"],\n",
    "    \"bird\": [\"bird\", \"parrot\", \"duck\", \"chirp\", \"avian\"],\n",
    "    \"dragon\": [\"dragon\", \"fire\", \"scale\", \"mythical\"],\n",
    "    \"penguin\": [\"penguin\", \"waddle\", \"antarctica\"],\n",
    "    \"fish\": [\"fish\", \"aquatic\", \"ocean\", \"fin\"],\n",
    "    \"crypto_parody\": [\"crypto\", \"parody\", \"satire\", \"spoof\"],\n",
    "    \"food\": [\"food\", \"snack\", \"burger\", \"pizza\", \"sushi\"],\n",
    "    \"anime\": [\"anime\", \"manga\", \"otaku\", \"japan\", \"waifu\"],\n",
    "    \"political\": [\"political\", \"president\", \"election\", \"government\", \"policy\"],\n",
    "    \"fantasy\": [\"fantasy\", \"magic\", \"wizard\", \"elf\", \"orc\"],\n",
    "    \"gorilla\": [\"gorilla\", \"ape\", \"monkey\", \"primate\"],\n",
    "    \"chad_chud_kek_4chan\": [\"chad\", \"chud\", \"sigma\", \"alpha\", \"giga\", \"4chan\", \"kek\", \"maximus\"],\n",
    "    \"mog\": [\"mog\", \"mogging\", \"dominance\", \"overpower\"],\n",
    "    \"chill_guy\": [\"chill\", \"guy\", \"relax\", \"vibe\", \"laidback\"],\n",
    "    \"peanut\": ['peanut', 'squirrel', 'peanutthesquirrel', 'peanut the squirrel', 'peanutthesquirrel'],\n",
    "    \"elon\": ['elon', 'musk', 'elonmusk', 'elon musk', 'elonmuskdoge', 'elonmuskdoge', \"grok\"],\n",
    "}\n",
    "\n",
    "embedding_dim = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "category_descriptions = [' '.join(keywords) for keywords in categories.values()]\n",
    "category_embeddings = model.encode(category_descriptions)\n",
    "category_embeddings = normalize(category_embeddings)\n",
    "faiss_index.add(category_embeddings)\n",
    "\n",
    "predicted_categories = []\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    embedding = embedding.reshape(1, -1) \n",
    "    distances, indices = faiss_index.search(embedding, 1)  # looks for the correct category to put it in\n",
    "    max_score = 1 - distances[0][0]  # similarity score\n",
    "    threshold = 0.5  # confidence needed to put into a category \n",
    "\n",
    "    if max_score >= threshold: # if we put it in the category\n",
    "        category = list(categories.keys())[indices[0][0]]\n",
    "        new_keywords = df['processed_description'].iloc[i].split()\n",
    "        categories[category].extend(new_keywords)\n",
    "        # we add the description from the new coin to the category, for learning\n",
    "        categories[category] = list(set(categories[category]))  \n",
    "        category_descriptions = [' '.join(keywords) for keywords in categories.values()]\n",
    "        category_embeddings = model.encode(category_descriptions)\n",
    "        category_embeddings = normalize(category_embeddings)\n",
    "        faiss_index.reset()  \n",
    "        faiss_index.add(category_embeddings)  \n",
    "    else: # else, we create a new category\n",
    "        new_category_name = f\"new_category_{len(categories)}\"\n",
    "        new_keywords = df['processed_description'].iloc[i].split()\n",
    "        categories[new_category_name] = new_keywords\n",
    "        category_descriptions = [' '.join(keywords) for keywords in categories.values()]\n",
    "        category_embeddings = model.encode(category_descriptions)\n",
    "        category_embeddings = normalize(category_embeddings)\n",
    "        faiss_index.reset()  \n",
    "        faiss_index.add(category_embeddings)  \n",
    "        category = new_category_name\n",
    "\n",
    "    predicted_categories.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id            meme_type  \\\n",
      "0        dogecoin                  dog   \n",
      "1       shiba-inu                  dog   \n",
      "2            pepe                 pepe   \n",
      "3            bonk                  wif   \n",
      "4  pudgy-penguins              penguin   \n",
      "5      dogwifcoin                  wif   \n",
      "6           ai16z                   ai   \n",
      "7           floki                 none   \n",
      "8     based-brett  chad_chud_kek_4chan   \n",
      "9        fartcoin            fart_butt   \n",
      "\n",
      "                                         description  \n",
      "0  Dogecoin is a cryptocurrency based on the popu...  \n",
      "1  Shiba Inu (SHIB) is a meme token which began a...  \n",
      "2  What is the project about?\\r\\nPepe is a commun...  \n",
      "3  Bonk is the first Solana dog coin for the peop...  \n",
      "4  PENGU is the official coin of Pudgy Penguins. ...  \n",
      "5  Literally a dog wif a hat, dogwifhat (WIF) is ...  \n",
      "6  ai16z is the first venture capital DAO led by ...  \n",
      "7  FLOKI is the utility token of the Floki Ecosys...  \n",
      "8  BRETT the dancer, gamer, and cultural icon of ...  \n",
      "9                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "df['meme_type'] = predicted_categories\n",
    "\n",
    "print(df[['id', 'meme_type', 'description']].head(10))  \n",
    "\n",
    "df.to_csv(\"classified_meme_coins.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
